# Part 3 -- AI for Toxicity Prediction

In this part we experiment on using Deep Learning and more classical machine learning techniques to classify compounds, or get a prediction for the LD50 outcome. We use the same dataset as in part 3, derived from the TAME toolbox. 
Deep Learning is a modern predictive approach where neural networks are used to train a model on labelled data. The network are able to detect pattern associated to the outcome, based on the label. There are many different architectures available to choose from. Depending on the intended task, one or more of these architectures can be chosen. Also, there are a number of implementations available to build neural networks. Popular frameworks are [PyTorch](https://pytorch.org/), [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/?gclid=EAIaIQobChMIptjygcrG-wIVyOZ3Ch1omQ4MEAAYASAAEgLtF_D_BwE). PyTorch is available for Python only, whereas Keras and Tensorflow are avaible for both Python and R programming environments. Because R is relatively much used by the academic community, we use tensorflow and keras in R for this workshop. The [`{reticulate}` R package](), which is an interface to Python from R, makes it possible to setup Tensorflow in RStudio. 

It can however be quite tricky to get this technically setup, especially when you are on a Mac with M1-chip. Luckily there is are many good resources to help you along:

 - https://developer.apple.com/metal/tensorflow-plugin/
 - https://tensorflow.rstudio.com/install/
 - https://stackoverflow.com/questions/50145643/unable-to-change-python-path-in-reticulate

For natural languge processing [Hugging Face](https://huggingface.co/) models are a good place to start.

I you are looking for a good resource to start on Deep Learning, see the excellent book from Manning that [is avaialble for R](https://www.manning.com/books/deep-learning-with-r) and for [Python](https://www.manning.com/books/deep-learning-with-python-second-edition)

## Installing Tensorflow for R
We follow the steps in theese [docs ](https://tensorflow.rstudio.com/install/) for installing Tensorflow on Windows computers. If you experience technical difficulties, we can try to help you during the workshop. We are also available after the workshop or via an online meeting to help you along if needed.

```{r include = FALSE, eval = TRUE}
# set CSS for objects
knitr::opts_chunk$set(
  class.source="Rchunk", 
  class.output="Rout", 
  warning = FALSE,
  error = FALSE,
  message = FALSE)
```

## Packages
```{r}
library(tensorflow)
library(tidyverse)
library(keras)
library(tfruns)
library(fs)
```

```{r include=FALSE}
load("course_urls.RData")
les <- 4
```

## Data
```{r}
substances <- read_csv(
  here::here(
    "data-raw",
    "substances.csv")) |>
  janitor::clean_names()

acute_data <- read_csv(
    here::here(
      "data-raw",
      "acute_tox_data.csv")) |>
  janitor::clean_names()

```

## Remove duplicated SMILES
Because we want to predict toxicity (class and continuous outcome of LD50_LM), we need to solve the issue of duplicated SMILES. Some smiles will have a label 'nontoxic', where that same smile will have a label 'toxic' when connected to a different compound. This can be problematic, because it will lead to ambiguous label - to - feature correlations.
To solve this, I decided to just remove all observations from the data that have a duplicated SMILES. This solution is not ideal though. It would need a more thorough search and find to connect the right label to the right compound.

```{r}
ind <- duplicated(substances$qsar_ready_smiles)
sum(ind)  

substances[ind,] -> x
acute_data[ind,] -> y

y |> group_by(nontoxic) |> tally()

## we remove the duplicates
substances <- substances[!ind, ]
acute_data <- acute_data[!ind, ]
```

## Preprocessing the data
Here we combine the data to contain the fingerprints, the `dtxsid` ids and the `nontoxic` and the `ld40_lm` column

```{r}
acute_data_select <- acute_data |>
  dplyr::select(
    dtxsid,
    nontoxic,
    ld50_lm
  )

substances_select <- substances |>
  dplyr::select(
    dtxsid,
    qsar_ready_smiles
  ) 

data_nn <- full_join(acute_data_select, substances_select)
```

We reproduce part of the previous section to convert the qsar ready smiles to fingerprints

```{r}
library(rcdk)
library(rcdklibs)
all_smiles <- substances_select$qsar_ready_smiles
all_mols <-parse.smiles(all_smiles)
all_mols[[1]]
```

### Computing chemical fingerprints

We can run the same function over the entire 'all_mols' dataset, leveraging the `map()` function from the `{purrr}` R package of the `{tidyverse}`:
```{r}
all.fp <- 
  map(all_mols, 
      get.fingerprint, 
      type='standard')

## Convert the pf list to a df
fp_tbl <- fingerprint::fp.to.matrix(all.fp) |> as_tibble()
## adding the predicted class (nontoxic as column)

fp_tbl <- fp_tbl |>
  mutate(
    class = ifelse(
      test = acute_data_select$nontoxic == "TRUE",  ## recode the class, 0 = nontoxic, 1 = toxic
      yes = 0,
      no = 1),
    ld50_lm = acute_data_select$ld50_lm) |>
  relocate(class, ld50_lm)

```

## Split data into training and test set
```{r}
library(rsample)

## seed for reproducibility
set.seed(123)
data_split <- initial_split(fp_tbl, prop = 3/4)

## trainig data
training_data <- training(data_split) |> 
  select(-c(class, ld50_lm)) |> 
  as.matrix() |>
  array_reshape(c(nrow(training(data_split)), 1*1024))

training_labels_class <- training(data_split) |> select(class) |> 
  as.matrix() |>
#  as.integer() |>
  array_reshape(nrow(training(data_split)))

## test data
test_data <- testing(data_split) |>
  select(-c(class, ld50_lm)) |> 
  as.matrix() |>
  array_reshape(c(nrow(testing(data_split)), 1*1024))

test_labels_class <- testing(data_split) |> select(class) |> 
  as.matrix() |>
# as.integer() |>
  array_reshape(nrow(testing(data_split)))
  
training_data[1,c(1:80)]
training_labels_class[1:10]
test_data[1,c(1:80)]
test_labels_class[1:10]
```

## Neural Network for binary classification
Here we use the fingerprints as 1D tensors, one tensor per compound
```{r}
set.seed(123)
model <- keras_model_sequential(input_shape = c(1*1024)) |>
  layer_dense(units = 1024, activation = "relu") %>%
  layer_dropout(rate = 0.5) |>
  layer_dense(units = 512, activation = "relu") |>
  layer_dense(units = 512, activation = "relu") |>
  layer_dropout(0.2) %>%
  layer_dense(units = 16, activation = "relu") |>
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

## store training in a history object so that we can see how the model is doing, also on a small validation set.
## validation split, means that 20% of the training data is reserved for validation.
history <- model %>% 
  fit(training_data, 
      training_labels_class,
      epochs = 50,
      verbose = 2,
  validation_split = 0.2
)

## we can plot the history
plot(history)

## evaluate our model on the test data
model %>% evaluate(test_data, test_labels_class, verbose = 2)

# Confusion matrix
pred <- model %>% predict(test_data, batch_size = 10)
y_pred = round(pred)
confusion_matrix = table(y_pred, test_labels_class)
confusion_matrix
```

## Optimizing the model
The model above is quite complex and big in terms of layers and number of neurons. When we look at the model on the validation set we can see in the plot that the model is overfitting. Let's start by reducing the size and complexity of the model first.
```{r}
set.seed(123)
## simpler models
model <- keras_model_sequential(input_shape = c(1*1024)) |>
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(rate = 0.9) |>
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(rate = 0.6) |>
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

## store training in a history object so that we can see how the model is doing, also on a small validation set.
## validation split, means that 20% of the training data is reserved for validation.
history <- model %>% 
  fit(training_data, 
      training_labels_class,
      epochs = 20,
      # Suppress logging.
      verbose = 2,
  # Calculate validation results on 20% of the training data.
  validation_split = 0.2
)

## we can plot the history
plot(history)

## evaluate our model on thetest data
model %>% evaluate(test_data, test_labels_class, verbose = 2) -> eval_metrics

# Confusion matrix
pred <- model %>% predict(test_data, batch_size = 10)
y_pred = round(pred)
confusion_matrix = table(y_pred, test_labels_class)
confusion_matrix


cat("Loss:", eval_metrics[[1]], "\n") 
cat("Accuracy:", eval_metrics[[2]], "\n")
```

## Tuning a neural network
We could keep on experimenting with different values for the different hyperparamters in the above network, but keeping manual track of the performance of all these permutations seems like a daunting and cumbersome task. Luckily, there is a better, more structured way to go about this. See for a full explanation [this tutorial on 'Tensorflow for R'](https://tensorflow.rstudio.com/guides/tfruns/tuning)

Below we reproduce this example, using our network. We will focus on tuning the dropout-values first. In oder to run multiple models, I extracted the code for loading the data, converting SMILES to fingerprints, preprocessing, data split and fitting the neural network model into an .R script called `./tf-nn-qsar.R`. To easily extract r code from and RMarkdown file, look at the `purl()` function from the `{knitr}` R package. (`?knitr::purl`).

Look at the file `./tf-nn-qsar.R` and find the `FLAGS`. These `FLAGS` are used to run iterations for multiple values of these `FLAGS` in the code below:

This computation takes a long time, so again we are loading the dataframe with the results from disk. The `sample = 0.1` argument in `tuning_run()`, will randomly sample 10% of all passible combinations for all chosen values of the `FLAGS`. In this case this leads to `r round((4*4*6*6)*0.1, 0)` combinations. This means that the `tf-nn-qsar.R` script will be run 58 times. I included the code but not as a code chunk, so that it is not accidentally run.

```{r}
## delete existing runs
if(dir.exists(here::here("runs"))){
  dir_delete(here::here("runs"))}

# run various combinations of dropout1 and dropout2, and units1 and units2 to find the best values
set.seed(123)
runs <- tuning_run("tf-nn-qsar.R", flags = list(
  dropout1 = c(0.2, 0.3, 0.6, 0.9),
  dropout2 = c(0.2, 0.3, 0.6, 0.9),
  units1 = c(8, 16, 28, 64, 128, 512),
  units2 = c(8, 16, 28, 64, 128, 512)
), sample = 0.1) # add sample to reduce number of combinations that will be tested

## write to disk
write_rds(
  runs,
  here::here(
    "data",
    "nn-tuning-runs.rds"
    )
  )
```

## Find the best evaluation accuracy
We could make a table ranked by validation (test) accuracy, and find the best values for the hyperparameters.

```{r}
## load runs from disk
runs <- read_rds(
  here::here(
    "data",
    "nn-tuning-runs.rds"
    )
)

runs |>
  dplyr::select(
    metric_val_accuracy,
    flag_dropout1,
    flag_dropout2,
    flag_units1,
    flag_units2
    ) |>
  arrange(desc(metric_val_accuracy)) |>
  head(1)

```

So, we can conclude that comopared to our manual efforts, a smaller network, with less units and less severe dropout yields the best predictive capacity (i.e. highest accuracy).

We can also visualize this.

```{r}
runs |>
  dplyr::select(
    metric_val_accuracy,
    flag_dropout1,
    flag_dropout2,
    flag_units1,
    flag_units2
    ) |>
  pivot_longer(
    flag_dropout1:flag_dropout2,
    values_to = "values",
    names_to = "dropouts"
  ) |>
  ggplot(
    aes(
      x = values,
      y = metric_val_accuracy)
  ) +
  geom_point(aes(colour = dropouts)) -> dropouts_viz

runs |>
  dplyr::select(
    metric_val_accuracy,
    flag_dropout1,
    flag_dropout2,
    flag_units1,
    flag_units2
    ) |>
  pivot_longer(
    flag_units1:flag_units2,
    values_to = "values",
    names_to = "units"
  ) |>
  ggplot(
    aes(
      x = values,
      y = metric_val_accuracy)
  ) +
  geom_point(aes(colour = units)) -> units_viz

cowplot::plot_grid(
  dropouts_viz,
  units_viz
)

```

To optimize the neural network hyperparameters, we could expand the grid to include a wider range of values. For now, I think it is fair to assume that we reached a plateau for the performance, on this data.

## Prediction of a continuous outcome (regression)
Here we will try to predict the LD50_LM score, on the basis of the chemical fingerprints as tensors.
Because this outcome is on a continuous scale, we need a slightly different model architecture. This type of modelling approach is considered a regression problem.
We will repeat some of the preprocessing steps, because we need a different outcome variable.

```{r, eval=FALSE}
## continuous outcome (training)
training_labels_ld50_lm <- training(data_split) |> 
  select(ld50_lm) |>
  as.matrix() |>
  array_reshape(nrow(training(data_split)))

## continuous outcome (testing)
test_labels_ld50_lm <- testing(data_split) |> select(ld50_lm) |> 
  as.matrix() |>
  array_reshape(nrow(testing(data_split)))
  
training_labels_ld50_lm[1:10]
test_labels_ld50_lm[1:10]
```

## Regression outcome prediction
We use the same model as above but with a different output layer. Also we specify a different outcome: MAE = Mean Absolute Error, and should be as low as possible.
```{r}
set.seed(123)
model <- keras_model_sequential(input_shape = c(1*1024)) |>
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.2) |>
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.1) |>
#  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("mae")
)

history <- model %>% 
  fit(training_data, 
      training_labels_ld50_lm,
      epochs = 20,
      # Suppress logging.
      verbose = 2,
  # Calculate validation results on 20% of the training data.
  validation_split = 0.2
)

plot(history)

model %>% evaluate(test_data,  test_labels_ld50_lm, verbose = 2)
model |> predict(test_data) -> predictions

predictions[1,]
realvalue <- test_labels_ld50_lm[1]
realvalue

## lets plot the correlations
predictions <- tibble(
  .pred = predictions,
  .real = test_labels_ld50_lm
)

origin <- tibble(
  x = seq(-3, 5, length.out = 9),
  y = seq(-3, 5, length.out = 9)
)

predictions |>
  ggplot(aes(x = .real, y = .pred)) +
  geom_point(shape = 1) +
  geom_line(data = origin, aes(x = x, y = y), colour = "darkred")

```

## What's next?
We could take several steps to try and improve our models. Here, I focus on the deep learning part:

 1. Use different embeddings: e.g. Graph embeddings, see https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Introduction_to_Graph_Convolutions.ipynb
 2. Extend the validation, using k-fold validation: see e.g Deep Learning with R, page 79
 3. Use different architecture
 4. Collect more training data
 5. Build an ensemble model (combining multiple models), see e.g. https://pubs.acs.org/doi/abs/10.1021/acs.chemrestox.9b00259
 6. Combine the Read-Across and Deep Learning approaches
 7. Expand the feature space to biological information
 
